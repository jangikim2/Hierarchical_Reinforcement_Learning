Our implementation is based on HIRO open code\textsuperscript{\ref{hiro_code}} and RealNVP\footnote[4]{\url{https://github.com/haarnoja/sac}} in \textcolor{black}{Latent}. We have released our code with explanations\footnote[5]{\url{https://github.com/jangikim2/Hierarchical_Reinforcement_Learning}}. The definition of a goal and the intrinsic reward function of a lower-level policy defined in HIRO are used without modification. Only the implementation of the lower-level policy and off-policy correction of the HIRO open code has been changed. In addition, the implementation of RealNVP in \textcolor{black}{Latent} is adopted to the HIRO open code. The two-level hierarchical structure of HRL is used because the same HRL structure of HIRO is used. In the Ant environment, the dimension of state $s_{t}$ and action $a_{t,z}$ is 30 and 8, respectively. \textcolor{black}{The default dimension of goal $g_{t}$ of the lower-level policy is 15.} \textcolor{black}{The default dimension of goal $g_{t}$ of the higher-level policy is 2 or 3 depending on the task.} The main parameters which are defined in our model for all tasks are the same as those \textcolor{black}{of} HIRO. All other neural networks including the critic of the FDGM part except for the actor of the FDGM part, which is a RealNVP, consist of a fully connected neural network as utilized in HIRO. \textcolor{black}{In order to find the best performance for each task, the parameters which are tuned for training our model are as follows.} \textcolor{black}{The training step used in this research is 10M steps.} When our model is compared with HIRO with the same parameters, the size of the higher-level policy and lower-level policy of HIRO is defined according to Algorithm \ref{alg:the_alg1} or \ref{alg:the_alg2}. Particularly in Algorithm \ref{alg:the_alg1}, the width size of the lower-level policy of HIRO \textcolor{black}{and Latent} with the same parameters \textcolor{black}{for the comparison} is the same as that of the actor of the FDGM part of our model. \textcolor{black}{Lastly, the $a_{t,z-\text{state}}$ of output of conditional part is also tuned.} \textcolor{black}{The size of all policies of 'Our model\textunderscore 8\textunderscore 8' in Fig. \ref{fig:Latent Result_4_task} is the same as that of 'Our model\textunderscore 8\textunderscore 15'. In Fig. \ref{fig:Latent Result_4_task_15_8}, the context range of the higher-level policy of 'Our model\textunderscore 2\textunderscore 8' and 'Our model\textunderscore 3\textunderscore 8' is the same as the original context range of Ant Push Single and Ant Fall Single of HIRO respectively. The size of all policies is the same as that of our model in Fig. \ref{fig:Latent Result_4_task}. In addition, the best size of $a_{t,z-\text{state}}$ for each experiment is chosen. As with Ant Fall Single of Fig. \ref{fig:Latent Result_4_task}, our model in Ant Fall Single takes advantage of the combination of the inverse operation of Algorithm \ref{alg:the_alg2} and the size of NN of Algorithm \ref{alg:the_alg1}. Latent takes advantage of the same inverse mechanism such as our model using RealNVP to find a goal for off-policy correction because it can support the fair comparison of experiment.}